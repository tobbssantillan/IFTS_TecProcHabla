# -*- coding: utf-8 -*-
"""TPH_spacy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apra7K7sLyfl_Z_nmSzUbnqt8NVYEHV8

## Análisis sobre suricatas - Extraído de Wikipedia
"""

# Descargar el modelo en español (solo si no está instalado)
!python -m spacy download es_core_news_lg -q

# --- Imports esenciales ---
import spacy
import es_core_news_lg
import en_core_web_sm

from spacy import displacy
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
import random
import re
import nltk
from nltk.corpus import stopwords

# --- Carga de modelos por idioma ---
nlp_es = es_core_news_lg.load()       # Modelo en español
nlp_en = en_core_web_sm.load()        # Modelo en inglés

# --- Carga de stopwords ---
nltk.download('stopwords')
stopwords_en = set(stopwords.words('english'))

texto_ejemplo = (
    "Meerkats are active during the day, mostly in the early morning and late afternoon; "
    "they remain continuously alert and retreat to burrows when sensing danger. "
    "They use a broad variety of calls to communicate among one another for different purposes, "
    "for example to raise an alarm on sighting a predator. "
    "Primarily insectivorous, meerkats feed heavily on beetles and lepidopterans, arthropods, "
    "amphibians, small birds, reptiles, and plant material in their diet."
)

# --- Procesamiento del texto en inglés ---
doc_en = nlp_en(texto_ejemplo)

# Tokenización
tokens = [token.text for token in doc_en]
print("Texto tokenizado:\n", tokens)

# Lematización (forma base de cada palabra)
print("Lemas de cada token:")
for token in doc_en:
    if not token.is_punct and not token.is_space:
        print(f"'{token.text}' → '{token.lemma_}'")

# Etiquetado gramatical (POS tagging)
print("Etiquetas gramaticales por token:")
for token in doc_en:
    if not token.is_space:
        print(f"'{token.text}' → {token.pos_} ({spacy.explain(token.pos_)}) / {token.tag_} ({spacy.explain(token.tag_)})")

# Análisis de dependencias sintácticas
print("Dependencias sintácticas:")

for token in doc_en:
  if not token.is_space:
    print (f"'{token.text}' -> {token.dep_} ({spacy.explain(token.dep_)} -> '{token.head.text}'")

# Visualización de dependencias.

displacy.render (doc_en, style='dep', jupyter=True, options={'distance': 120})

if doc_en.ents:
  print("Entidades encontradas:")
  print("Texto de la Entidad - > Etiqueta(Tipo)")
  for ent in doc_en.ents:
    print(f"'{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})")
else:
  print ("No se encontraron entidades nombradas en este texto")

displacy.render(doc_en, style='ent', jupyter=True)

"""## Análisis de texto extraído sobre Genghis Kan"""

wiki_txt = """
El Imperio mongol fue instituido por Genghis Kan en 1206.
Tras largas luchas internas, unificó a las diversas tribus mongolas bajo su mando, involucrándolas en una expansión que les llevaría a conquistar China, Asia Central, Rusia y llegar hasta Irak, Siria y Anatolia.
También les llevaría a la incursión de Europa central y del resto de la Europa Oriental para poder consolidar la conquista de Rusia.
Así llegaron, entre otros sitios, temporalmente a Dalmacia, que estaba controlada por Venecia, lo que puso a los venecianos por primera vez en contacto directo con los mongoles.
A su muerte le sucedió su hijo Ogodei, quien continuó con esta expansión y consolidó la jerarquía del Gran Kan sobre los diversos reinos mongoles. En tiempos de Marco Polo este Gran Kan era Kublai Kan.​
El dominio del Imperio mongol no solo fue militar, sino también cultural y comercial.
Durante su expansión, los mongoles establecieron una red de rutas comerciales seguras conocida como la Pax Mongolica, que permitió el florecimiento del comercio entre Oriente y Occidente.
A lo largo de la Ruta de la Seda, bienes, ideas, tecnología e incluso enfermedades se difundieron con mayor rapidez que nunca antes.
Uno de los aspectos más notables del Imperio mongol fue su tolerancia religiosa. Aunque Genghis Kan seguía creencias chamánicas tradicionales, permitió la práctica de diversas religiones dentro de sus dominios, incluyendo el islam, el budismo, el cristianismo nestoriano y el confucianismo.
Esto fomentó una relativa estabilidad social en los vastos territorios conquistados.
Kublai Kan, nieto de Genghis Kan, estableció la dinastía Yuan en China, convirtiéndose en emperador y trasladando la capital a lo que hoy es Pekín.
Bajo su gobierno, se integraron muchas prácticas administrativas chinas al sistema mongol, y se promovió la reconstrucción y desarrollo económico del país tras las guerras de conquista.
La corte de Kublai Kan recibió a embajadores, comerciantes y viajeros de todo el mundo, incluido el famoso Marco Polo, quien pasó varios años al servicio del Gran Kan.
Sus relatos, aunque a veces vistos con escepticismo, ofrecieron a Europa una de las primeras descripciones detalladas de Asia.
Sin embargo, tras la muerte de Kublai Kan, el imperio comenzó a fragmentarse, y los distintos kanatos (como el Kanato de la Horda de Oro, el Ilkanato o el Kanato de Chagatai) siguieron caminos separados, muchas veces en conflicto entre sí.
Esta división debilitó el poder central y facilitó la recuperación de territorios por parte de pueblos conquistados.
A pesar de su colapso gradual, la huella del Imperio mongol fue profunda. Dejó un legado en las estructuras políticas, las comunicaciones y las relaciones interculturales que moldearon el desarrollo de Eurasia durante siglos posteriores.
"""

print(f"'{wiki_txt[1:270]}'") #Muestra un fragmento

doc_es = nlp_es(wiki_txt)
palabras_clave = []

for token in doc_es:
  if token.is_alpha and not token.is_stop: # Si no es stop word o alfanumérico
    # Normalización: Obtener lema y convertir a minúsculas
    palabras_clave.append(token.lemma_.lower())

print(f"Se extrajeron {len(palabras_clave)} palabras clave (lemas, sin stop words).")
# Ejemplo de las primeras palabras extraídas
print (f"Ejemplo: {palabras_clave[:15]}")

frecuencia_palabras = Counter(palabras_clave)
N = 10
palabras_mas_comunes = frecuencia_palabras.most_common(N)
for palabra, frecuencia in palabras_mas_comunes:
  print (f"- '{palabra} : {frecuencia}")

wordcloud_generator = WordCloud(
    width=800,
    height=400,
    background_color='antiquewhite',
    colormap='copper', # Paleta de colores
    max_words=50,      # Mostrar máximo 50 palabras
    stopwords=None,    # Ya filtramos stop words antes
    collocations=False # Evitar que agrupe palabras (ej. "imperio mongol")
                       # Si se quisiera, sería mejor hacerlo antes con Spacy/N-gramas
).generate_from_frequencies(frecuencia_palabras) # <-- Usar las frecuencias calculadas

plt.figure(figsize=(10, 5)) # Tamaño de la figura donde se mostrará
plt.imshow(wordcloud_generator, interpolation='bilinear') # Mostrar la imagen generada
plt.axis("off") # No mostrar los ejes X e Y
plt.tight_layout(pad=0) # Ajustar para que no haya bordes extra
plt.show() # <-- ¡Mostrar la ventana con la nube!

"""# Análisis de párrafo Los Viajes de Marco Polo Vol. II"""

# Guardamos la URL del texto en variable "url"

url = "https://www.gutenberg.org/cache/epub/12410/pg12410-images.html"

# Petición al sitio web

contenido = requests.get(url).text

# Creamos el objeto BeautifulSoup para analizar el HTML

soup = BeautifulSoup(contenido, "html.parser")

# Buscamos todos los títulos <h1>

h1 = soup.find_all("h1")
print("El título del libro es: ", h1)

parrafo = soup.find_all("p")
parrafo_aleatorio = random.choice(parrafo).text.strip()
print("Elegimos párrafo aleatorio: \n ", parrafo_aleatorio)
print(" Cantidad de palabras en el párrafo: ", len(parrafo_aleatorio.split()))

doc_parrafo = nlp_en(parrafo_aleatorio)
print ("¡Texto procesado!")

palabras_clave_url = []

for token_url in doc_parrafo:
  if token_url.is_alpha and not token_url.is_stop: # Si no es stop word o alfanumérico
    # Normalización: Obtener lema y convertir a minúsculas
    palabras_clave_url.append(token_url.lemma_.lower())

print(f"Se extrajeron {len(palabras_clave_url)} palabras clave (lemas, sin stop words).")
# Ejemplo de las primeras palabras extraídas
print (f"Ejemplo: {palabras_clave_url[:100]}")

frecuencia_palabras_url = Counter(palabras_clave_url)
N = 15
palabras_mas_comunes_url = frecuencia_palabras_url.most_common(N)
for palabras, frecuencias in palabras_mas_comunes_url:
  print (f"- '{palabras} : {frecuencias}")

# Visualización de dependencias.

displacy.render (doc_parrafo, style='dep', jupyter=True, options={'distance': 120})

if doc_parrafo.ents:
  print("Entidades encontradas:")
  print("Texto de la Entidad - > Etiqueta(Tipo)")
  for ent in doc_parrafo.ents:
    print(f"'{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})")
else:
  print ("No se encontraron entidades nombradas en este texto")

displacy.render(doc_parrafo, style='ent', jupyter=True)

"""# Análisis de texto entero de Los Viajes de Marco Polo Vol. II"""

libro = soup.find_all("p")
libro_analisis = ""

for parrafo in libro:
  libro_analisis += parrafo.text.strip() + "\n"

libro_analisis = re.sub(r"[^a-zA-Z0-9\s]", "", libro_analisis)  # Remove non-alphanumeric characters
libro_analisis = libro_analisis.lower()  # Convert to lowercase

print ("El libro contiene", len(libro_analisis), "cantidad de caracteres.")
nlp_en.max_length = 2000000 # Aumentamos la cantidad de caracteres disponibles para poder analizar

doc_libro = nlp_en(libro_analisis)
print("¡Libro procesado con spaCy!")

palabras_clave_libro = []
for token in doc_libro:
  if token.is_alpha and not token.is_stop:
    palabras_clave_libro.append(token.lemma_.lower())

print(f"Se extrajeron {len(palabras_clave_libro)} palabras clave del libro.")

frecuencia_palabras_libro = Counter(palabras_clave_libro)

N = 10  # Define la cantidad de palabras más comunes que deseas obtener
palabras_mas_comunes_libro = frecuencia_palabras_libro.most_common(N)
for palabra, frecuencia in palabras_mas_comunes_libro:
  print(f"- '{palabra}': {frecuencia}")

palabras_a_eliminar = [
    'say', 'come', 'see', 'look', 'shall', 'speak', 'find', 'think',
    'give', 'believe', 'said', 'hear', 'know', 'go', 'shall', 'tell',
    'one', 'must', 'may', 'made', 'took', 'came', 'seemed', 'saw', 'thing',
    'though', 'upon', 'heard', 'seen', 'might', 'back', 'went', 'matter',
    'dont', 'well', 'sat', 'make', 'began', 'thought', 'looked', 'wouldnt',
    'still', 'take', 'even', 'keep', 'set', 'much', 'turn', 'toward', 'put',
    'p', 'de', 'en', 'n', 'j', 'le', 'g', 'dr', 'etc', 'pp', 'e', 'iii',
    'ad', 'la', 'ie', 'c', 'v', 'ii', 'u', 'du', 'ch', '8vo', 'h', 'et',
    'oldnumber', 'fu', 'vol', 'di', 'des',
        # ... agrega más palabras que quieras eliminar
]

def limpiar_texto_con_blacklist(texto, blacklist):
  black_list = stopwords_en.union(blacklist)
  texto_minuscula = texto.lower()
  texto_sin_puntuacion = re.sub(r'[^\w\s]', '', texto_minuscula)
  lista_palabras = texto_sin_puntuacion.split()
  palabras_filtradas = []

  for palabra in lista_palabras:
    if palabra not in black_list:
      palabras_filtradas.append(palabra)
  return palabras_filtradas

palabras_limpias = limpiar_texto_con_blacklist(libro_analisis, palabras_a_eliminar)
texto_limpio = ' '.join(palabras_limpias)

wordcloud = WordCloud(
    width=800,
    height=400,
    background_color="floralwhite",
    colormap = "copper",
    max_words=100,
    collocations=False

).generate(texto_limpio)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()
